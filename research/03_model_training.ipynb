{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    root_dir: Path\n",
    "    trained_model_path: Path\n",
    "    updated_base_model_path: Path\n",
    "    training_data: Path\n",
    "    params_epochs: int\n",
    "    params_batch_size: int\n",
    "    params_is_augmentation: bool\n",
    "    params_image_size: list\n",
    "    params_data_size: int\n",
    "    trained_stopwords_path:Path\n",
    "    trained_vectorizer_path:Path\n",
    "    trained_stemmer_path: Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentiment_analysis.constants import *\n",
    "from sentiment_analysis.utils.common import read_yaml, create_directories\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "        \n",
    "\n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        training = self.config.training\n",
    "        prepare_base_model = self.config.prepare_base_model\n",
    "        params = self.params\n",
    "        # training_data = os.path.join(self.config.data_ingestion.unzip_dir, \"Chest--Scan-data\")\n",
    "        training_data = os.path.join(self.config.data_ingestion.unzip_dir, \"Reviews.csv\")\n",
    "        create_directories([\n",
    "            Path(training.root_dir)\n",
    "        ])\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            root_dir=Path(training.root_dir),\n",
    "            trained_model_path=Path(training.trained_model_path),\n",
    "            trained_stopwords_path=Path(training.trained_stop_words_path),\n",
    "            trained_vectorizer_path = Path(training.trained_vectorizer_path),\n",
    "            trained_stemmer_path=Path(training.trained_stemmer_path),\n",
    "            updated_base_model_path=Path(prepare_base_model.updated_base_model_path),\n",
    "            training_data=Path(training_data),\n",
    "            params_epochs=params.EPOCHS,\n",
    "            params_batch_size=params.BATCH_SIZE,\n",
    "            params_is_augmentation=params.AUGMENTATION,\n",
    "            params_image_size=params.IMAGE_SIZE,\n",
    "            params_data_size=params.DATA_SIZE\n",
    "\n",
    "        )\n",
    "\n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request as request\n",
    "from zipfile import ZipFile\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/abhisekh.agarwala/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abhisekh.agarwala/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "plt.style.use('ggplot')\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "\n",
    "    \n",
    "    def get_base_model(self):\n",
    "        # self.model = tf.keras.models.load_model(\n",
    "        #     self.config.updated_base_model_path\n",
    "        # )\n",
    "        self.model = LogisticRegression()\n",
    "\n",
    "    def train_valid_generator(self):\n",
    "\n",
    "        # datagenerator_kwargs = dict(\n",
    "        #     rescale = 1./255,\n",
    "        #     validation_split=0.20\n",
    "        # )\n",
    "\n",
    "        # dataflow_kwargs = dict(\n",
    "        #     target_size=self.config.params_image_size[:-1],\n",
    "        #     batch_size=self.config.params_batch_size,\n",
    "        #     interpolation=\"bilinear\"\n",
    "        # )\n",
    "\n",
    "        # valid_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        #     **datagenerator_kwargs\n",
    "        # )\n",
    "\n",
    "        # self.valid_generator = valid_datagenerator.flow_from_directory(\n",
    "        #     directory=self.config.training_data,\n",
    "        #     subset=\"validation\",\n",
    "        #     shuffle=False,\n",
    "        #     **dataflow_kwargs\n",
    "        # )\n",
    "\n",
    "        # if self.config.params_is_augmentation:\n",
    "        #     train_datagenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        #         rotation_range=40,\n",
    "        #         horizontal_flip=True,\n",
    "        #         width_shift_range=0.2,\n",
    "        #         height_shift_range=0.2,\n",
    "        #         shear_range=0.2,\n",
    "        #         zoom_range=0.2,\n",
    "        #         **datagenerator_kwargs\n",
    "        #     )\n",
    "        # else:\n",
    "        #     train_datagenerator = valid_datagenerator\n",
    "\n",
    "        # self.train_generator = train_datagenerator.flow_from_directory(\n",
    "        #     directory=self.config.training_data,\n",
    "        #     subset=\"training\",\n",
    "        #     shuffle=True,\n",
    "        #     **dataflow_kwargs\n",
    "        # )\n",
    "        pass\n",
    "\n",
    "    def training_data_preparation(self):\n",
    "        df = pd.read_csv(self.config.training_data)\n",
    "        df = df[['Text', 'Score']]\n",
    "        df = df.sample(n=self.config.params_data_size)\n",
    "        df = df.loc[df['Score']!=3]\n",
    "        df = df.loc[df['Score']!=4]\n",
    "        def category(score):\n",
    "            return 0 if score==1 or score==2 else 1\n",
    "        df['Sentiment']= df['Score'].apply(category)\n",
    "        self.stop_words = stopwords.words('english')\n",
    "        self.stemmer = PorterStemmer()\n",
    "        def text_preprocessing(text):\n",
    "            lower_casing = text.lower()\n",
    "            tokens = word_tokenize(lower_casing)\n",
    "            tokens = [self.stemmer.stem(token) for token in tokens if token not in self.stop_words and token not in string.punctuation]\n",
    "            return \" \".join(tokens)\n",
    "        \n",
    "        df['Text'] = df['Text'].apply(text_preprocessing)\n",
    "        \n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(df['Text'], df['Sentiment'], test_size=0.2)\n",
    "\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "\n",
    "        self.X_train_vect = self.vectorizer.fit_transform(self.X_train)\n",
    "        \n",
    "        self.X_test_vect = self.vectorizer.transform(self.X_test)\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_model(path: Path, model: tf.keras.Model):\n",
    "        # model.save(path)\n",
    "        pass\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_artifacts(obj, path: Path):\n",
    "        with open(path,'wb') as f:\n",
    "            pickle.dump(obj,f)\n",
    "\n",
    "\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        # self.steps_per_epoch = self.train_generator.samples // self.train_generator.batch_size\n",
    "        # self.validation_steps = self.valid_generator.samples // self.valid_generator.batch_size\n",
    "\n",
    "        # self.model.fit(\n",
    "        #     self.train_generator,\n",
    "        #     epochs=self.config.params_epochs,\n",
    "        #     steps_per_epoch=self.steps_per_epoch,\n",
    "        #     validation_steps=self.validation_steps,\n",
    "        #     validation_data=self.valid_generator\n",
    "        # )\n",
    "\n",
    "        # self.save_model(\n",
    "        #     path=self.config.trained_model_path,\n",
    "        #     model=self.model\n",
    "        # )\n",
    "\n",
    "        self.model.fit(self.X_train_vect, self.y_train)\n",
    "        predictions = self.model.predict(self.X_test_vect)\n",
    "        accuracy_score = accuracy_score(self.y_test, predictions)\n",
    "        confusion_metrix = confusion_matrix(y_test, y_pred)\n",
    "        logger.info(f\"model accuracy {accuracy_score}\")\n",
    "        logger.infor(f\"model confusion matrix{confusion_metrix}\")\n",
    "        self.save_artifacts(self.model, self.config.trained_model_path)\n",
    "        self.save_artifacts(self.stop_words, self.config.trained_stopwords_path)\n",
    "        self.save_artifacts(self.stemmer, self.config.trained_stemmer_path)\n",
    "        self.save_artifacts(self.vectorizer, self.config.trained_vectorizer_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-06 23:02:59,052: Sentiment-Analysis: INFO: common.py: read_yaml:- yaml file: config/config.yaml loaded successfully\n",
      "2024-08-06 23:02:59,053: Sentiment-Analysis: INFO: common.py: read_yaml:- yaml file: params.yaml loaded successfully\n",
      "2024-08-06 23:02:59,054: Sentiment-Analysis: INFO: common.py: create_directories:- created directory at: artifacts\n",
      "2024-08-06 23:02:59,054: Sentiment-Analysis: INFO: common.py: create_directories:- created directory at: artifacts/training\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'accuracy_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     training\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[28], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# training.train_valid_generator()\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     training\u001b[38;5;241m.\u001b[39mtraining_data_preparation()\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[27], line 116\u001b[0m, in \u001b[0;36mTraining.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train_vect, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_train)\n\u001b[1;32m    115\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_test_vect)\n\u001b[0;32m--> 116\u001b[0m accuracy_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccuracy_score\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my_test, predictions)\n\u001b[1;32m    117\u001b[0m confusion_metrix \u001b[38;5;241m=\u001b[39m confusion_matrix(y_test, y_pred)\n\u001b[1;32m    118\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel accuracy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'accuracy_score'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    training_config = config.get_training_config()\n",
    "    training = Training(config=training_config)\n",
    "    training.get_base_model()\n",
    "    # training.train_valid_generator()\n",
    "    training.training_data_preparation()\n",
    "    training.train()\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
